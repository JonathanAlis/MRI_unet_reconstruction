{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe1662828f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "import torch, gc\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "from models import Unet, get_default_device, to_device, DeviceDataLoader\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L2', 'L1', 'TV', 'L2_L1', 'L2_TV', 'L1_TV', 'L2_L1_TV']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rectype=['L2','L1','TV']\n",
    "rectype_combinations=[] \n",
    "rectype_strings=[]\n",
    "for i in range(len(rectype)):\n",
    "    for p in combinations(rectype, i+1):  # 2 for pairs, 3 for triplets, etc\n",
    "\n",
    "        rectype_combinations.append(p)\n",
    "        rectype_strings.append('_'.join(p))\n",
    "rectype_combinations\n",
    "rectype_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=[1e-4,'exp', 'plateau']\n",
    "radial_lines=[20,40,60,80,100]\n",
    "max_epochs=500\n",
    "batch_sizes=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./BIRN_dataset/birn_pngs_20lines_L2/',\n",
       " './BIRN_dataset/birn_pngs_40lines_L2/',\n",
       " './BIRN_dataset/birn_pngs_60lines_L2/',\n",
       " './BIRN_dataset/birn_pngs_80lines_L2/',\n",
       " './BIRN_dataset/birn_pngs_100lines_L2/',\n",
       " './BIRN_dataset/birn_pngs_20lines_L1/',\n",
       " './BIRN_dataset/birn_pngs_40lines_L1/',\n",
       " './BIRN_dataset/birn_pngs_60lines_L1/',\n",
       " './BIRN_dataset/birn_pngs_80lines_L1/',\n",
       " './BIRN_dataset/birn_pngs_100lines_L1/',\n",
       " './BIRN_dataset/birn_pngs_20lines_TV/',\n",
       " './BIRN_dataset/birn_pngs_40lines_TV/',\n",
       " './BIRN_dataset/birn_pngs_60lines_TV/',\n",
       " './BIRN_dataset/birn_pngs_80lines_TV/',\n",
       " './BIRN_dataset/birn_pngs_100lines_TV/']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir='./BIRN_dataset/'\n",
    "images_dir=(dataset_dir+'birn_png/')\n",
    "rec_dirs=[(f\"{dataset_dir}birn_pngs_{rl}lines_{rt}/\") for rt in rectype for rl in radial_lines]\n",
    "rec_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OriginalReconstructionDataset(Dataset):\n",
    "    def __init__(self, radial_line, rec_type_str, datasets_dir, indexes = None, img_size=(256,256)):\n",
    "        rec_type=rec_type_str.split('_')\n",
    "        self.images_dir=(dataset_dir+'birn_png/')\n",
    "        rec_dirs=[(f\"{dataset_dir}birn_pngs_{rl}lines_{rt}/\") for rt in rectype for rl in radial_lines]\n",
    "        \n",
    "        self.rec_images_dirs=[]\n",
    "        for dir in rec_dirs:\n",
    "            for rt in rec_type:\n",
    "                if rt in dir:\n",
    "                    if str(radial_line) in dir:\n",
    "                        self.rec_images_dirs.append(dir)\n",
    "                        break\n",
    "\n",
    "        self.images = [f for f in os.listdir(self.images_dir) if f.endswith('.png')]\n",
    "        if indexes is not None:\n",
    "            self.images = [self.images[i] for i in indexes] \n",
    "        self.transform = transforms.Compose([\n",
    "                        transforms.Grayscale(num_output_channels=1),         \n",
    "                        transforms.Resize(img_size),\n",
    "                        #transforms.Lambda(lambda x: x/255.0),\n",
    "                        transforms.ToTensor()\n",
    "                        ])\n",
    "        self.rec_types=rec_type\n",
    "        self.radial_line=radial_line\n",
    "        print(self.images_dir)\n",
    "        print(self.rec_images_dirs)\n",
    "        print(self.rec_types)\n",
    "        print(self.radial_line)\n",
    "    def __len__(self):\n",
    "    # return length of image samples    \n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name=self.images[idx]\n",
    "        img = Image.open(self.images_dir+img_name)\n",
    "        img=self.transform(img)\n",
    "        rec_imgs=[]\n",
    "        for rec,dir in zip(self.rec_types,self.rec_images_dirs):\n",
    "            noisy_name=img_name[:-14]+rec+f'_{self.radial_line}lines.png'            \n",
    "            tensor=self.transform(Image.open(dir+noisy_name))\n",
    "            rec_imgs.append(tensor)\n",
    "        noisy=torch.stack(rec_imgs)\n",
    "        noisy=torch.squeeze(noisy, 1)\n",
    "        return (img,noisy)\n",
    "\n",
    "def first_element(test_dataset):\n",
    "    for data in test_dataset:\n",
    "        print(data[0].shape)\n",
    "        print(data[1].shape)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_file='indexes.pkl'\n",
    "if not os.path.exists(idx_file):\n",
    "    np.random.seed(seed=42)\n",
    "    all_indexes=np.random.permutation(len([f for f in os.listdir(images_dir) if f.endswith('.png')]))\n",
    "    m = len(all_indexes)\n",
    "    m_train=int(m*0.8)\n",
    "    m_val = int(m*0.1)\n",
    "    train_indexes=all_indexes[:m_train]\n",
    "    val_indexes=all_indexes[m_train:m_train+m_val]\n",
    "    test_indexes=all_indexes[m_train+m_val:]\n",
    "    \n",
    "    with open(idx_file, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "        pickle.dump([train_indexes, val_indexes, test_indexes], f)\n",
    "else:\n",
    "    with open(idx_file,'rb') as f:  # Python 3: open(..., 'rb')\n",
    "        train_indexes, val_indexes, test_indexes = pickle.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/']\n",
      "['L2']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/']\n",
      "['L2']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/']\n",
      "['L2']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/']\n",
      "['L2']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/']\n",
      "['L2']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L1/']\n",
      "['L1']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L1/']\n",
      "['L1']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L1/']\n",
      "['L1']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L1/']\n",
      "['L1']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L1/']\n",
      "['L1']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/', './BIRN_dataset/birn_pngs_20lines_L1/']\n",
      "['L2', 'L1']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/', './BIRN_dataset/birn_pngs_40lines_L1/']\n",
      "['L2', 'L1']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/', './BIRN_dataset/birn_pngs_60lines_L1/']\n",
      "['L2', 'L1']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/', './BIRN_dataset/birn_pngs_80lines_L1/']\n",
      "['L2', 'L1']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/', './BIRN_dataset/birn_pngs_100lines_L1/']\n",
      "['L2', 'L1']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/', './BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['L2', 'TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/', './BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['L2', 'TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/', './BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['L2', 'TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/', './BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['L2', 'TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/', './BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['L2', 'TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L1/', './BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['L1', 'TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L1/', './BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['L1', 'TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L1/', './BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['L1', 'TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L1/', './BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['L1', 'TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L1/', './BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['L1', 'TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/', './BIRN_dataset/birn_pngs_20lines_L1/', './BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/', './BIRN_dataset/birn_pngs_40lines_L1/', './BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/', './BIRN_dataset/birn_pngs_60lines_L1/', './BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/', './BIRN_dataset/birn_pngs_80lines_L1/', './BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/', './BIRN_dataset/birn_pngs_100lines_L1/', './BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/']\n",
      "['L2']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/']\n",
      "['L2']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/']\n",
      "['L2']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/']\n",
      "['L2']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/']\n",
      "['L2']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L1/']\n",
      "['L1']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L1/']\n",
      "['L1']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L1/']\n",
      "['L1']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L1/']\n",
      "['L1']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L1/']\n",
      "['L1']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/', './BIRN_dataset/birn_pngs_20lines_L1/']\n",
      "['L2', 'L1']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/', './BIRN_dataset/birn_pngs_40lines_L1/']\n",
      "['L2', 'L1']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/', './BIRN_dataset/birn_pngs_60lines_L1/']\n",
      "['L2', 'L1']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/', './BIRN_dataset/birn_pngs_80lines_L1/']\n",
      "['L2', 'L1']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/', './BIRN_dataset/birn_pngs_100lines_L1/']\n",
      "['L2', 'L1']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/', './BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['L2', 'TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/', './BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['L2', 'TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/', './BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['L2', 'TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/', './BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['L2', 'TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/', './BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['L2', 'TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L1/', './BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['L1', 'TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L1/', './BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['L1', 'TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L1/', './BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['L1', 'TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L1/', './BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['L1', 'TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L1/', './BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['L1', 'TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/', './BIRN_dataset/birn_pngs_20lines_L1/', './BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/', './BIRN_dataset/birn_pngs_40lines_L1/', './BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/', './BIRN_dataset/birn_pngs_60lines_L1/', './BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/', './BIRN_dataset/birn_pngs_80lines_L1/', './BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/', './BIRN_dataset/birn_pngs_100lines_L1/', './BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/']\n",
      "['L2']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/']\n",
      "['L2']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/']\n",
      "['L2']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/']\n",
      "['L2']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/']\n",
      "['L2']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L1/']\n",
      "['L1']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L1/']\n",
      "['L1']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L1/']\n",
      "['L1']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L1/']\n",
      "['L1']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L1/']\n",
      "['L1']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/', './BIRN_dataset/birn_pngs_20lines_L1/']\n",
      "['L2', 'L1']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/', './BIRN_dataset/birn_pngs_40lines_L1/']\n",
      "['L2', 'L1']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/', './BIRN_dataset/birn_pngs_60lines_L1/']\n",
      "['L2', 'L1']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/', './BIRN_dataset/birn_pngs_80lines_L1/']\n",
      "['L2', 'L1']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/', './BIRN_dataset/birn_pngs_100lines_L1/']\n",
      "['L2', 'L1']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/', './BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['L2', 'TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/', './BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['L2', 'TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/', './BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['L2', 'TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/', './BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['L2', 'TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/', './BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['L2', 'TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L1/', './BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['L1', 'TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L1/', './BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['L1', 'TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L1/', './BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['L1', 'TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L1/', './BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['L1', 'TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L1/', './BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['L1', 'TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/', './BIRN_dataset/birn_pngs_20lines_L1/', './BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/', './BIRN_dataset/birn_pngs_40lines_L1/', './BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/', './BIRN_dataset/birn_pngs_60lines_L1/', './BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/', './BIRN_dataset/birn_pngs_80lines_L1/', './BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/', './BIRN_dataset/birn_pngs_100lines_L1/', './BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "batch_size=4\n",
    "device=get_default_device()\n",
    "\n",
    "train_dataset={}\n",
    "train_loaders={}\n",
    "for rt in rectype_strings:\n",
    "    for rl in radial_lines:\n",
    "        train_ds=OriginalReconstructionDataset(rl, rt, dataset_dir, train_indexes)\n",
    "        train_dataset[rl,rt]=train_ds\n",
    "        train_loaders[rl,rt]=DeviceDataLoader(torch.utils.data.DataLoader(train_ds, batch_size=batch_size), device)\n",
    "        first_element(train_ds)\n",
    "\n",
    "\n",
    "val_dataset={}\n",
    "val_loaders={}\n",
    "for rt in rectype_strings:\n",
    "    for rl in radial_lines:\n",
    "        val_ds=OriginalReconstructionDataset(rl, rt, dataset_dir, val_indexes)\n",
    "        val_dataset[rl,rt]=val_ds\n",
    "        val_loaders[rl,rt]=DeviceDataLoader(torch.utils.data.DataLoader(val_ds, batch_size=batch_size), device)\n",
    "        first_element(val_ds)\n",
    "\n",
    "test_dataset={}\n",
    "test_loaders={}\n",
    "for rt in rectype_strings:\n",
    "    for rl in radial_lines:\n",
    "        test_ds=OriginalReconstructionDataset(rl, rt, dataset_dir, test_indexes)\n",
    "        test_dataset[rl,rt]=test_ds\n",
    "        test_loaders[rl,rt]=DeviceDataLoader(torch.utils.data.DataLoader(test_ds, batch_size=batch_size,shuffle=True), device)\n",
    "        first_element(test_ds)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_oprimizer(lr, params_to_optimize):\n",
    "    if lr=='exp':\n",
    "        optimizer = torch.optim.Adam(params_to_optimize, lr=0.001, weight_decay=1e-05)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.000001**(1/max_epochs), last_epoch=- 1, verbose=False)\n",
    "    elif lr=='plateau':\n",
    "        optimizer = torch.optim.Adam(params_to_optimize, lr=0.001, weight_decay=1e-05)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1**(1/2), patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Training function\n",
    "def train_epoch_den(model, device, dataloader, loss_fn, optimizer):\n",
    "    # Set train mode\n",
    "    model.train()    \n",
    "    train_loss = []\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for image_batch, image_noisy in tqdm(dataloader): # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "        image_noisy.to(device)\n",
    "        #print(device)\n",
    "        #print((image_noisy.device))\n",
    "        \n",
    "        result = model(image_noisy)\n",
    "        # Evaluate loss\n",
    "        loss = loss_fn(result, image_batch)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #if lr=='exp' or lr=='plateau':\n",
    "        #    scheduler.step()        \n",
    "        # Print batch loss\n",
    "        #print('\\t partial train loss (single batch): %f' % (loss.data))\n",
    "        train_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "### Testing function\n",
    "def test_epoch_den(model, device, dataloader, loss_fn):\n",
    "    # Set evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # Define the lists to store the outputs for each batch\n",
    "        conc_out = []\n",
    "        conc_label = []\n",
    "        for image_batch, image_noisy in dataloader:\n",
    "            result = model(image_noisy)\n",
    "            # Append the network output and the original image to the lists\n",
    "            conc_out.append(result.cpu())\n",
    "            conc_label.append(image_batch.cpu())\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        conc_out = torch.cat(conc_out)\n",
    "        conc_label = torch.cat(conc_label) \n",
    "        # Evaluate global loss\n",
    "        val_loss = loss_fn(conc_out, conc_label)\n",
    "    return val_loss.data\n",
    "\n",
    "def plot_ae_outputs_den(unet,n=10):\n",
    "    plt.figure(figsize=(21,6))\n",
    "    for i in range(n):\n",
    "\n",
    "      ax = plt.subplot(3,n,i+1)\n",
    "      img = test_dataset[4*i][0].unsqueeze(0)\n",
    "      image_noisy = test_dataset[4*i][1].unsqueeze(0)\n",
    "      \n",
    "      unet.eval()\n",
    "\n",
    "      with torch.no_grad():\n",
    "         rec_img  = unet(image_noisy)\n",
    "\n",
    "      plt.imshow(img.cpu().squeeze().numpy()[0,:,:], cmap='gist_gray')\n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "        ax.set_title('Original images')\n",
    "      ax = plt.subplot(3, n, i + 1 + n)\n",
    "      plt.imshow(image_noisy.cpu().squeeze().numpy()[0,:,:], cmap='gist_gray')\n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "        ax.set_title('Corrupted images')\n",
    "\n",
    "      ax = plt.subplot(3, n, i + 1 + n + n)\n",
    "      plt.imshow(rec_img.cpu().squeeze().numpy()[0,:,:], cmap='gist_gray')  \n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "         ax.set_title('Reconstructed images')\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.7, \n",
    "                    top=0.9, \n",
    "                    wspace=0.3, \n",
    "                    hspace=0.3)   \n",
    "    \n",
    "    plt.savefig('images_256_CS_TV.png')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(rec_type, radial_lines, model, loss_fn, optimizer, dl_train, dl_val,max_epochs):#train_loaders[rl,rt]=DeviceDataLoader(torch.utils.data.DataLoader(train_ds, batch_size=batch_size), device)\n",
    "    history={'learning_rate':[],'train_loss':[],'val_loss':[]}\n",
    "    last_saved=''\n",
    "    best_model=''\n",
    "    for epoch in range(max_epochs):\n",
    "        print('EPOCH %d/%d' % (epoch + 1, max_epochs))\n",
    "        ### Training (use the training function)\n",
    "        train_loss=train_epoch_den(\n",
    "            model=model, \n",
    "            device=device, \n",
    "            dataloader=dl_train, \n",
    "            loss_fn=loss_fn, \n",
    "            optimizer=optimizer)\n",
    "        ### Validation  (use the testing function)\n",
    "        val_loss = test_epoch_den(\n",
    "            model=model, \n",
    "            device=device, \n",
    "            dataloader=dl_val,\n",
    "            loss_fn=loss_fn)\n",
    "        # Print Validationloss\n",
    "\n",
    "\n",
    "        train_loss=train_loss if not torch.is_tensor(train_loss) else train_loss.cpu().detach().numpy()\n",
    "        val_loss=val_loss if not torch.is_tensor(val_loss) else val_loss.cpu().detach().numpy()\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['learning_rate'].append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        print('\\n EPOCH {}/{} \\t train loss {:.8f} \\t val loss {:.8f}'.format(epoch + 1, max_epochs,train_loss,val_loss))\n",
    "        if val_loss<=min(history['val_loss']):\n",
    "            print('Lowest val loss:',val_loss, '... Saving parameters.')\n",
    "            try:\n",
    "                os.remove(last_saved)\n",
    "            except:\n",
    "                print('couldnt remove {last_saved}')\n",
    "            torch.save(model.state_dict(), f'saved_models/unet_{radial_lines}{rectype}_epoch{epoch + 1}_lr{lr}.pth')\n",
    "            last_saved= f'unet_{radial_lines}{rectype}_epoch{epoch + 1}_lr{lr}.pth'\n",
    "        break\n",
    "        \n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(history)\n",
    "    df.to_csv(f'saved_models/loss_history_{radial_lines}{rectype}_epoch{epoch + 1}_lr{lr}.csv')\n",
    "\n",
    "    return history\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 2 µs, total: 10 µs\n",
      "Wall time: 12.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import mlflow\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def execute(model, num_radial_lines, data_train, data_val, data_test, device, exp_params):\n",
    "    experiment_name=f\"MRIREC_{num_radial_lines}\"\n",
    "    run_params = {\"description\":f\"Reconstruction using {num_radial_lines} radial lines\",\n",
    "              \"tags\":{'release.version':'1.0.0'}}\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if not experiment:\n",
    "        experiment_id=mlflow.create_experiment(experiment_name)\n",
    "    experiment = mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    run_params.update({\"experiment_id\": experiment.experiment_id})\n",
    "    \n",
    "    print(\"Experiment_id: {}\".format(experiment.experiment_id))\n",
    "    print(\"Localização dos artefatos: {}\".format(experiment.artifact_location))\n",
    "    print(\"Tags: {}\".format(experiment.tags))\n",
    "    print(\"Lifecycle_stage: {}\".format(experiment.lifecycle_stage))\n",
    "\n",
    "    grid_exp = ParameterGrid(exp_params)\n",
    "\n",
    "    for p_model in grid_exp:\n",
    "        print(p_model)\n",
    "        model = p_model['model']\n",
    "        rectype = p_model['rectype']\n",
    "        num_channels = len(rectype.split('_'))\n",
    "        epochs = p_model['max_epochs']\n",
    "        learnig_rate=p_model['learnig_rate']\n",
    "        batch_size = p_model['batch_size']\n",
    "        train_ds= OriginalReconstructionDataset(num_radial_lines, rectype, dataset_dir, train_indexes)\n",
    "        train_dl= DeviceDataLoader(torch.utils.data.DataLoader(train_ds, batch_size=batch_size), device)\n",
    "        val_ds=OriginalReconstructionDataset(num_radial_lines, rectype, dataset_dir, test_indexes)\n",
    "        val_dl= DeviceDataLoader(torch.utils.data.DataLoader(train_ds, batch_size=batch_size), device)\n",
    "\n",
    "        #loading model\n",
    "        \n",
    "        if model=='Unet':\n",
    "            model=Unet(num_inputs=len(rectype.split('_'))) #1 a 3 canais\n",
    "        elif False:\n",
    "            pass\n",
    "            #IF TO ADD NEW MODEL IMPLEMENT H#RE\n",
    "        else:\n",
    "            error('Model not found...')\n",
    "        model.to(device)\n",
    "        \n",
    "        params_to_optimize = [{'params': model.parameters()}]        \n",
    "        optimizer=define_optimizer(p_model.lr, params_to_optimize)\n",
    "\n",
    "        #train loop:\n",
    "        break\n",
    "        train(rectype, num_radial_lines, model, torch.nn.MSELoss(), optimizer,train_dl, val_dl, device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment_id: 585268919028592994\n",
      "Localização dos artefatos: file:///home/jonathan/MRI_unet_reconstruction/mlruns/585268919028592994\n",
      "Tags: {}\n",
      "Lifecycle_stage: active\n",
      "{'batch_size': 4, 'learnig_rate': 0.0001, 'model': 'Unet', 'rectype': 'L2'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m radial_lines\u001b[39m=\u001b[39m[\u001b[39m20\u001b[39m,\u001b[39m40\u001b[39m,\u001b[39m60\u001b[39m,\u001b[39m80\u001b[39m,\u001b[39m100\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m rl \u001b[39min\u001b[39;00m radial_lines:\n\u001b[0;32m---> 12\u001b[0m     execute(models, rl, train_loaders, val_loaders, test_loaders,device,exp_params)\n",
      "File \u001b[0;32m<timed exec>:28\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(model, num_radial_lines, data_train, data_val, data_test, device, exp_params)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lr'"
     ]
    }
   ],
   "source": [
    "\n",
    "models=\n",
    "\n",
    "exp_params={\"model\": ['Unet'],\n",
    "            \"rectype\": rectype_strings,\n",
    "            \"learnig_rate\":[1e-4, 'exp', 'plateau'],\n",
    "            \"max_epochs\":[500],\n",
    "            \"batch_size\": [4],\n",
    "}\n",
    "\n",
    "\n",
    "radial_lines=[20,40,60,80,100]\n",
    "\n",
    "for rl in radial_lines:\n",
    "    execute(models, rl, train_loaders, val_loaders, test_loaders,device,exp_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
