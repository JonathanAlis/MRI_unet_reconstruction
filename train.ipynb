{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f993a2eaa70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "import torch, gc\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "from models import Unet, get_default_device, to_device, DeviceDataLoader\n",
    "from models import ResnetUnet, ConvUNeXt\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L2', 'L1', 'TV', 'L2_L1', 'L2_TV', 'L1_TV', 'L2_L1_TV']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rectype=['L2','L1','TV']\n",
    "rectype_combinations=[] \n",
    "rectype_strings=[]\n",
    "for i in range(len(rectype)):\n",
    "    for p in combinations(rectype, i+1):  # 2 for pairs, 3 for triplets, etc\n",
    "\n",
    "        rectype_combinations.append(p)\n",
    "        rectype_strings.append('_'.join(p))\n",
    "rectype_combinations\n",
    "rectype_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./BIRN_dataset/birn_pngs_20lines_L2/',\n",
       " './BIRN_dataset/birn_pngs_40lines_L2/',\n",
       " './BIRN_dataset/birn_pngs_60lines_L2/',\n",
       " './BIRN_dataset/birn_pngs_80lines_L2/',\n",
       " './BIRN_dataset/birn_pngs_100lines_L2/',\n",
       " './BIRN_dataset/birn_pngs_20lines_L1/',\n",
       " './BIRN_dataset/birn_pngs_40lines_L1/',\n",
       " './BIRN_dataset/birn_pngs_60lines_L1/',\n",
       " './BIRN_dataset/birn_pngs_80lines_L1/',\n",
       " './BIRN_dataset/birn_pngs_100lines_L1/',\n",
       " './BIRN_dataset/birn_pngs_20lines_TV/',\n",
       " './BIRN_dataset/birn_pngs_40lines_TV/',\n",
       " './BIRN_dataset/birn_pngs_60lines_TV/',\n",
       " './BIRN_dataset/birn_pngs_80lines_TV/',\n",
       " './BIRN_dataset/birn_pngs_100lines_TV/']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radial_lines=[20,40,60,80,100]\n",
    "dataset_dir='./BIRN_dataset/'\n",
    "images_dir=(dataset_dir+'birn_png/')\n",
    "rec_dirs=[(f\"{dataset_dir}birn_pngs_{rl}lines_{rt}/\") for rt in rectype for rl in radial_lines]\n",
    "rec_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OriginalReconstructionDataset(Dataset):\n",
    "    def __init__(self, radial_line, rec_type_str, datasets_dir, indexes = None, img_size=(256,256)):\n",
    "        rec_type=rec_type_str.split('_')\n",
    "        self.images_dir=(dataset_dir+'birn_png/')\n",
    "        rec_dirs=[(f\"{dataset_dir}birn_pngs_{rl}lines_{rt}/\") for rt in rectype for rl in radial_lines]\n",
    "        \n",
    "        self.rec_images_dirs=[]\n",
    "        for dir in rec_dirs:\n",
    "            for rt in rec_type:\n",
    "                if rt in dir:\n",
    "                    if str(radial_line) in dir:\n",
    "                        self.rec_images_dirs.append(dir)\n",
    "                        break\n",
    "\n",
    "        self.images = [f for f in os.listdir(self.images_dir) if f.endswith('.png')]\n",
    "        if indexes is not None:\n",
    "            self.images = [self.images[i] for i in indexes] \n",
    "        self.transform = transforms.Compose([\n",
    "                        transforms.Grayscale(num_output_channels=1),         \n",
    "                        transforms.Resize(img_size),\n",
    "                        #transforms.Lambda(lambda x: x/255.0),\n",
    "                        transforms.ToTensor()\n",
    "                        ])\n",
    "        self.rec_types=rec_type\n",
    "        self.radial_line=radial_line\n",
    "        \n",
    "    def __len__(self):\n",
    "    # return length of image samples    \n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name=self.images[idx]\n",
    "        img = Image.open(self.images_dir+img_name)\n",
    "        img=self.transform(img)#.half()\n",
    "        rec_imgs=[]\n",
    "        for rec,dir in zip(self.rec_types,self.rec_images_dirs):\n",
    "            noisy_name=img_name[:-14]+rec+f'_{self.radial_line}lines.png'            \n",
    "            tensor=self.transform(Image.open(dir+noisy_name))\n",
    "            rec_imgs.append(tensor)\n",
    "        noisy=torch.stack(rec_imgs)\n",
    "        noisy=torch.squeeze(noisy, 1)#.half()\n",
    "        return (img,noisy)\n",
    "\n",
    "def first_element(test_dataset):\n",
    "    for data in test_dataset:\n",
    "        print(data[0].shape)\n",
    "        print(data[1].shape)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_file='indexes.pkl'\n",
    "if not os.path.exists(idx_file):\n",
    "    np.random.seed(seed=42)\n",
    "    all_indexes=np.random.permutation(len([f for f in os.listdir(images_dir) if f.endswith('.png')]))\n",
    "    m = len(all_indexes)\n",
    "    m_train=int(m*0.8)\n",
    "    m_val = int(m*0.1)\n",
    "    train_indexes=all_indexes[:m_train]\n",
    "    val_indexes=all_indexes[m_train:m_train+m_val]\n",
    "    test_indexes=all_indexes[m_train+m_val:]\n",
    "    \n",
    "    with open(idx_file, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "        pickle.dump([train_indexes, val_indexes, test_indexes], f)\n",
    "else:\n",
    "    with open(idx_file,'rb') as f:  # Python 3: open(..., 'rb')\n",
    "        train_indexes, val_indexes, test_indexes = pickle.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "#nao precisa dessa celula\n",
    "batch_size=4\n",
    "device=get_default_device()\n",
    "radial_lines=[20,40,60,80,100]\n",
    "\n",
    "train_dataset={}\n",
    "train_loaders={}\n",
    "for rt in rectype_strings:\n",
    "    for rl in radial_lines:\n",
    "        train_ds=OriginalReconstructionDataset(rl, rt, dataset_dir, train_indexes)\n",
    "        train_dataset[rl,rt]=train_ds\n",
    "        train_loaders[rl,rt]=DeviceDataLoader(torch.utils.data.DataLoader(train_ds, batch_size=batch_size), device)\n",
    "        first_element(train_ds)\n",
    "\n",
    "\n",
    "val_dataset={}\n",
    "val_loaders={}\n",
    "for rt in rectype_strings:\n",
    "    for rl in radial_lines:\n",
    "        val_ds=OriginalReconstructionDataset(rl, rt, dataset_dir, val_indexes)\n",
    "        val_dataset[rl,rt]=val_ds\n",
    "        val_loaders[rl,rt]=DeviceDataLoader(torch.utils.data.DataLoader(val_ds, batch_size=batch_size), device)\n",
    "        first_element(val_ds)\n",
    "\n",
    "test_dataset={}\n",
    "test_loaders={}\n",
    "for rt in rectype_strings:\n",
    "    for rl in radial_lines:\n",
    "        test_ds=OriginalReconstructionDataset(rl, rt, dataset_dir, test_indexes)\n",
    "        test_dataset[rl,rt]=test_ds\n",
    "        test_loaders[rl,rt]=DeviceDataLoader(torch.utils.data.DataLoader(test_ds, batch_size=batch_size,shuffle=True), device)\n",
    "        first_element(test_ds)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Training function\n",
    "def train_epoch_den(model, dataloader, optimizer, scheduler):\n",
    "    # Set train mode\n",
    "    loss_fn=nn.MSELoss(reduction='sum')\n",
    "    model.train()    \n",
    "    train_loss = 0\n",
    "    datasize=0\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for image_batch, image_noisy in tqdm(dataloader): # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "        #image_noisy.to(device)\n",
    "        result = model(image_noisy)\n",
    "        # Evaluate loss        \n",
    "        loss = loss_fn(result, image_batch)\n",
    "        # Backward pass        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        train_loss+=loss.detach().cpu().numpy()\n",
    "        datasize+=image_batch.shape[0]\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    return train_loss/datasize, optimizer, scheduler\n",
    "\n",
    "### Testing function\n",
    "def test_epoch_den(model, dataloader):\n",
    "    loss_fn=nn.MSELoss(reduction='sum')\n",
    "    # Set evaluation mode\n",
    "    model.eval()\n",
    "    val_loss=0\n",
    "    datasize=0\n",
    "\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        for image_batch, image_noisy in tqdm(dataloader):\n",
    "            result = model(image_noisy)\n",
    "            val_loss += loss_fn(result, image_batch)\n",
    "            datasize += image_batch.shape[0]\n",
    "\n",
    "    return val_loss/datasize\n",
    "\n",
    "def plot_ae_outputs_den(model,n=10):\n",
    "    plt.figure(figsize=(21,6))\n",
    "    for i in range(n):\n",
    "\n",
    "      ax = plt.subplot(3,n,i+1)\n",
    "      img = test_dataset[4*i][0].unsqueeze(0)\n",
    "      image_noisy = test_dataset[4*i][1].unsqueeze(0)\n",
    "      \n",
    "      model.eval()\n",
    "\n",
    "      with torch.no_grad():\n",
    "         rec_img  = model(image_noisy)\n",
    "\n",
    "      plt.imshow(img.cpu().squeeze().numpy()[0,:,:], cmap='gist_gray')\n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "        ax.set_title('Original images')\n",
    "      ax = plt.subplot(3, n, i + 1 + n)\n",
    "      plt.imshow(image_noisy.cpu().squeeze().numpy()[0,:,:], cmap='gist_gray')\n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "        ax.set_title('Corrupted images')\n",
    "\n",
    "      ax = plt.subplot(3, n, i + 1 + n + n)\n",
    "      plt.imshow(rec_img.cpu().squeeze().numpy()[0,:,:], cmap='gist_gray')  \n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "         ax.set_title('Reconstructed images')\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.7, \n",
    "                    top=0.9, \n",
    "                    wspace=0.3, \n",
    "                    hspace=0.3)   \n",
    "    \n",
    "    plt.savefig('images_256_CS_TV.png')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_optimizer(lr_type, lr, params_to_optimize,max_epochs=500):\n",
    "    if lr_type == 'constant':\n",
    "        optimizer = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
    "        scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
    "    if lr_type == 'step10':\n",
    "        optimizer = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)        \n",
    "    if lr_type =='exp':\n",
    "        optimizer  = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.000001**(1/max_epochs), last_epoch=- 1, verbose=False)\n",
    "    elif lr_type == 'plateau':\n",
    "        optimizer = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1**(1/2), patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
    "        scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
    "    return (lr_type, optimizer,scheduler)\n",
    "    \n",
    "    \n",
    "def save_model(path, epoch, model, optimizer, scheduler, train_loss, val_loss):\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            }, path)\n",
    "            \n",
    "def optimizer_to(optim, device):\n",
    "    for param in optim.state.values():\n",
    "        # Not sure there are any global tensors in the state dict\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)\n",
    "        elif isinstance(param, dict):\n",
    "            for subparam in param.values():\n",
    "                if isinstance(subparam, torch.Tensor):\n",
    "                    subparam.data = subparam.data.to(device)\n",
    "                    if subparam._grad is not None:\n",
    "                        subparam._grad.data = subparam._grad.data.to(device)\n",
    "\n",
    "def load_model(model_name, rec_type='', lr_type='constant', learning_rate=1e-4, path='', device='cuda:0'):\n",
    "    if model_name=='Unet':\n",
    "        model=Unet(num_inputs=len(rec_type.split('_'))) #1 a 3 canais\n",
    "    elif model_name=='ResnetUnet':        \n",
    "        model = ResnetUnet(in_channels=len(rec_type.split('_')))\n",
    "    elif model_name=='ConvUNeXt':\n",
    "        model = ConvUNeXt(in_channels=len(rec_type.split('_')),num_classes=1)\n",
    "        #IF TO ADD NEW MODEL IMPLEMENT H#RE\n",
    "    else:\n",
    "        raise Exception('Model not found...')\n",
    "    #model=model.half()\n",
    "    params_to_optimize = [{'params': model.parameters()}]        \n",
    "    lr_type, optimizer,scheduler=define_optimizer(lr_type, learning_rate, params_to_optimize)    \n",
    "    epoch = 0\n",
    "    train_loss=float('inf')\n",
    "    val_loss=float('inf')\n",
    "    \n",
    "    if path!='':\n",
    "        checkpoint = torch.load(path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        train_loss = checkpoint['train_loss']\n",
    "        val_loss = checkpoint['val_loss']\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    return (model, optimizer, scheduler, epoch, train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_csv(csv_filename):\n",
    "    if not os.path.exists(csv_filename):\n",
    "        df_history = pd.DataFrame(columns = ['epoch','lr_scheduler','learning_rate','train_loss','val_loss', 'checkpoint', 'status'])\n",
    "        last_epoch=0\n",
    "        last_checkpoint=''\n",
    "        return df_history, last_epoch, last_checkpoint\n",
    "    folder=csv_filename[:csv_filename.rfind('/')]\n",
    "    lr = float(csv_filename[csv_filename.find('LR_')+3:csv_filename.find('.csv')])\n",
    "    #preparing the csv\n",
    "    df_history=pd.read_csv(csv_filename)\n",
    "    df_history = df_history.loc[:, ~df_history.columns.str.contains('^Unnamed')]\n",
    "    df_history=df_history.drop_duplicates(subset=['epoch'], keep='first')\n",
    "\n",
    "    #find best model (lowest validation loss)\n",
    "    val_sorted_df=df_history.sort_values(by=['val_loss'])\n",
    "    for i, row in val_sorted_df.iterrows():\n",
    "        if os.path.isfile(row['checkpoint']):\n",
    "            best_epoch=row['epoch']\n",
    "            best_checkpoint=row['checkpoint']\n",
    "            break\n",
    "    \n",
    "    #find last saved model\n",
    "    epoch_sorted_df=df_history.sort_values(by=['epoch'], ascending=False)\n",
    "    for i, row in epoch_sorted_df.iterrows():\n",
    "        if os.path.isfile(row['checkpoint']):\n",
    "            last_epoch=row['epoch']\n",
    "            last_checkpoint=row['checkpoint']\n",
    "            break\n",
    "    \n",
    "    #erase rows that do not have a saved model\n",
    "    df_history=df_history[df_history['epoch']<=last_epoch]\n",
    "\n",
    "    files_2_delete=[folder+'/'+f for f in os.listdir(folder) if f.endswith(f'{lr}.pth')]\n",
    "    if best_checkpoint in files_2_delete:\n",
    "        files_2_delete.remove(best_checkpoint)\n",
    "    if last_checkpoint in files_2_delete:\n",
    "        files_2_delete.remove(last_checkpoint)    \n",
    "\n",
    "    for f in files_2_delete:\n",
    "        try:\n",
    "            os.remove(f)            \n",
    "        except:\n",
    "            pass#print(f'couldnt remove {f}')\n",
    "\n",
    "    return df_history, last_epoch, last_checkpoint\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(rec_type, radial_lines, model_name, lr_type, learning_rate, dl_train, dl_val,device,max_epochs):\n",
    "   \n",
    "    path=f'experiments/{model_name}_{radial_lines}lines_{rec_type}'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    csv_filename= f'{path}/loss_history_{lr_type}LR_{learning_rate}.csv'\n",
    "    df_history, last_epoch, last_checkpoint = check_csv(csv_filename)\n",
    "\n",
    "    (model, optimizer, scheduler, epoch, train_loss, val_loss) = load_model(model_name,  rec_type, lr_type, learning_rate, path=last_checkpoint, device=device)\n",
    "\n",
    "    for epoch in range(epoch,max_epochs-1):\n",
    "\n",
    "        print('EPOCH %d/%d' % (epoch, max_epochs))        \n",
    "        t_train=time.time()\n",
    "        ### Training (use the training function)\n",
    "        train_loss, optimizer, scheduler=train_epoch_den(\n",
    "            model=model, \n",
    "            dataloader=dl_train, \n",
    "            optimizer=optimizer, \n",
    "            scheduler=scheduler)\n",
    "        t_train=time.time()-t_train\n",
    "        t_val=time.time()\n",
    "        ### Validation  (use the testing function)\n",
    "        val_loss = test_epoch_den(\n",
    "            model=model, \n",
    "            dataloader=dl_val,\n",
    "            )\n",
    "        t_val=time.time()-t_val\n",
    "\n",
    "        train_loss=train_loss if not torch.is_tensor(train_loss) else train_loss.cpu().detach().numpy()\n",
    "        val_loss=val_loss if not torch.is_tensor(val_loss) else val_loss.cpu().detach().numpy()\n",
    "\n",
    "        filename=f'{path}/{model.name()}_{radial_lines}lines_{rec_type}_epoch{epoch}_{lr_type}LR_{scheduler.get_last_lr()[0]}.pth'        \n",
    "        \n",
    "        minimum_value=df_history['val_loss'].min() if len(df_history)>0 else float('inf')\n",
    "\n",
    "        if val_loss<=minimum_value:           \n",
    "            save_model(filename, epoch, model, optimizer, scheduler, train_loss, val_loss)\n",
    "            status='new best'\n",
    "            df_history.replace(to_replace=\"new best\", value='old best')\n",
    "            best_model=filename\n",
    "            print(f'new best loss: {val_loss}')\n",
    "        else:\n",
    "            status='never best'    \n",
    "        print(scheduler.get_lr())\n",
    "        print(scheduler.get_last_lr())\n",
    "        print(scheduler.state_dict())\n",
    "        df_history = pd.concat([df_history,\n",
    "                                  pd.DataFrame({'epoch':[epoch],\n",
    "                                                'lr_scheduler':[lr_type],\n",
    "                                                'learning_rate':[scheduler.get_last_lr()[0]],#optimizer.param_groups[0][\"lr\"]],\n",
    "                                                'train_loss':[train_loss],\n",
    "                                                'val_loss':[val_loss], \n",
    "                                                'checkpoint':[filename], \n",
    "                                                'status':[status],\n",
    "                                                'time_train':[t_train],\n",
    "                                                'time_val':[t_val]\n",
    "                                                })])\n",
    "        df_history = df_history.loc[:, ~df_history.columns.str.contains('^Unnamed')]\n",
    "        \n",
    "        df_history.to_csv(csv_filename)       \n",
    "        last_model=filename       \n",
    "\n",
    "\n",
    "    return df_history\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 µs, sys: 0 ns, total: 16 µs\n",
      "Wall time: 19.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import mlflow\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def execute(num_radial_lines, data_train, data_val, data_test, device, exp_params):\n",
    "    experiment_name=f\"MRIREC_{num_radial_lines}\"\n",
    "    run_params = {\"description\":f\"Reconstruction using {num_radial_lines} radial lines\",\n",
    "              \"tags\":{'release.version':'1.0.0'}}\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if not experiment:\n",
    "        experiment_id=mlflow.create_experiment(experiment_name)\n",
    "    experiment = mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    run_params.update({\"experiment_id\": experiment.experiment_id})\n",
    "    \n",
    "    print(\"Experiment_id: {}\".format(experiment.experiment_id))\n",
    "    print(\"Localização dos artefatos: {}\".format(experiment.artifact_location))\n",
    "    print(\"Tags: {}\".format(experiment.tags))\n",
    "    print(\"Lifecycle_stage: {}\".format(experiment.lifecycle_stage))\n",
    "\n",
    "    grid_exp = ParameterGrid(exp_params)\n",
    "\n",
    "    for p_model in grid_exp:\n",
    "        with mlflow.start_run(**run_params) as run:\n",
    "\n",
    "            mlflow.log_params({'model': p_model['model'], 'rectype': p_model['rectype'], 'max_epochs': p_model['max_epochs'], 'learning_rate': p_model['learning_rate'], 'batch_size': p_model['batch_size']})\n",
    "\n",
    "            print(p_model)\n",
    "            model = p_model['model']\n",
    "            rectype = p_model['rectype']\n",
    "            num_channels = len(rectype.split('_'))\n",
    "            epochs = p_model['max_epochs']\n",
    "            lr_type, learning_rate = p_model['learning_rate']\n",
    "            batch_size = p_model['batch_size']\n",
    "\n",
    "            train_ds= OriginalReconstructionDataset(num_radial_lines, rectype, dataset_dir, train_indexes)\n",
    "            train_dl= DeviceDataLoader(torch.utils.data.DataLoader(train_ds, batch_size=batch_size), device)\n",
    "            val_ds=OriginalReconstructionDataset(num_radial_lines, rectype, dataset_dir, test_indexes)\n",
    "            val_dl= DeviceDataLoader(torch.utils.data.DataLoader(train_ds, batch_size=batch_size), device)\n",
    "\n",
    "            #train loop:\n",
    "            history=train(rectype, num_radial_lines, model, lr_type, learning_rate, train_dl, val_dl, device,epochs)\n",
    "            \n",
    "            #display(history)\n",
    "            #print(history.val_loss.dtype)\n",
    "            #print(type(history.val_loss[0]))\n",
    "            #chosen_idx=history[['val_loss']].idxmin()[0]\n",
    "\n",
    "            #mlflow.log_metric('best_train_loss',history.at[chosen_idx,'train_loss'])\n",
    "            #mlflow.log_metric('best_val_loss',history.at[chosen_idx,'val_loss'])\n",
    "            #mlflow.log_metric('epoch',history.at[chosen_idx,'epoch'])\n",
    "            #mlflow.log_metric('checkpoint',history.at[chosen_idx,'checkpoint'])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment_id: 589449198711370880\n",
      "Localização dos artefatos: file:///home/jonathan/MRI_unet_reconstruction/mlruns/589449198711370880\n",
      "Tags: {}\n",
      "Lifecycle_stage: active\n",
      "{'batch_size': 2, 'learning_rate': ('constant', 0.001), 'max_epochs': 30, 'model': 'Unet', 'rectype': 'L2'}\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'last_epoch' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m radial_lines\u001b[39m=\u001b[39m[\u001b[39m20\u001b[39m,\u001b[39m40\u001b[39m,\u001b[39m60\u001b[39m,\u001b[39m80\u001b[39m,\u001b[39m100\u001b[39m]\n\u001b[1;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m rl \u001b[39min\u001b[39;00m radial_lines:\n\u001b[0;32m---> 29\u001b[0m     execute(rl, train_loaders, val_loaders, test_loaders,device,exp_params)\n",
      "File \u001b[0;32m<timed exec>:41\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(num_radial_lines, data_train, data_val, data_test, device, exp_params)\u001b[0m\n",
      "Cell \u001b[0;32mIn[24], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(rec_type, radial_lines, model_name, lr_type, learning_rate, dl_train, dl_val, device, max_epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(path)\n\u001b[1;32m      7\u001b[0m csv_filename\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m/loss_history_\u001b[39m\u001b[39m{\u001b[39;00mlr_type\u001b[39m}\u001b[39;00m\u001b[39mLR_\u001b[39m\u001b[39m{\u001b[39;00mlearning_rate\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m df_history, last_epoch, last_checkpoint \u001b[39m=\u001b[39m check_csv(csv_filename)\n\u001b[1;32m     10\u001b[0m (model, optimizer, scheduler, epoch, train_loss, val_loss) \u001b[39m=\u001b[39m load_model(model_name,  rec_type, lr_type, learning_rate, path\u001b[39m=\u001b[39mlast_checkpoint, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch,max_epochs\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n",
      "Cell \u001b[0;32mIn[23], line 31\u001b[0m, in \u001b[0;36mcheck_csv\u001b[0;34m(csv_filename)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m#erase rows that do not have a saved model\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m df_history\u001b[39m=\u001b[39mdf_history[df_history[\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m<\u001b[39m\u001b[39m=\u001b[39mlast_epoch]\n\u001b[1;32m     33\u001b[0m files_2_delete\u001b[39m=\u001b[39m[folder\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mf \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(folder) \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mendswith(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mlr\u001b[39m}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m best_checkpoint \u001b[39min\u001b[39;00m files_2_delete:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'last_epoch' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "#Paper: Cyclical Learning Rates for Training Neural Networks\n",
    "#PyTorch Lightning PyTorch Lightning (PL) [13] is a framework which decouples scientific components and engineering details in the code written for PyTorch [34]. PL enables our implementations\n",
    "#of the CSL approaches to be hardware agnostic, more easily readable, and accessible to researchers\n",
    "##with lower computational resources since it enables running the same code on arbitrary hardware. In\n",
    "#addition, it allows us to use the exactly same dataset splits, same fine-tuning protocol, early-stopping\n",
    "#criterion and transformation pipelines to ensure the consistency across various experimental settings.\n",
    "\n",
    "#Pytorch:\n",
    "#Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\n",
    "#Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative\n",
    "#style, high-performance deep learning library. In Advances in neural information processing\n",
    "#systems, pages 8026–8037, 2019.\n",
    "\n",
    "\n",
    "#Pytorch lightning: WA Falcon. Pytorch lightning. GitHub. Note: https://github. com/williamFalcon/pytorchlightning Cited by, 3, 2019\n",
    "\n",
    "exp_params={\"model\": ['Unet','ResnetUnet', 'ConvUNeXt'],\n",
    "            #\"model\": ['ResnetUnet'],\n",
    "            \"rectype\": rectype_strings,\n",
    "            \"learning_rate\":[('step10',1e-3)],#('constant', 1e-5)],#[1e-4],#, 'exp', 'plateau'],\n",
    "            \"max_epochs\":[30],\n",
    "            \"batch_size\": [2],\n",
    "}\n",
    "\n",
    "\n",
    "radial_lines=[20,40,60,80,100]\n",
    "\n",
    "for rl in radial_lines:\n",
    "    execute(rl, train_loaders, val_loaders, test_loaders,device,exp_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1.0000000000000002e-06\n",
      "1.0000000000000002e-06\n",
      "1.0000000000000002e-06\n",
      "1.0000000000000002e-06\n",
      "1.0000000000000002e-06\n",
      "1.0000000000000002e-06\n",
      "1.0000000000000002e-06\n",
      "1.0000000000000002e-06\n",
      "1.0000000000000002e-06\n",
      "1.0000000000000002e-06\n",
      "1.0000000000000002e-07\n",
      "1.0000000000000002e-07\n",
      "1.0000000000000002e-07\n",
      "1.0000000000000002e-07\n",
      "1.0000000000000002e-07\n",
      "1.0000000000000002e-07\n",
      "1.0000000000000002e-07\n",
      "1.0000000000000002e-07\n",
      "1.0000000000000002e-07\n",
      "1.0000000000000002e-07\n",
      "1.0000000000000004e-08\n",
      "1.0000000000000004e-08\n",
      "1.0000000000000004e-08\n",
      "1.0000000000000004e-08\n",
      "1.0000000000000004e-08\n",
      "1.0000000000000004e-08\n",
      "1.0000000000000004e-08\n",
      "1.0000000000000004e-08\n",
      "1.0000000000000004e-08\n",
      "1.0000000000000004e-08\n",
      "1.0000000000000005e-09\n",
      "1.0000000000000005e-09\n",
      "1.0000000000000005e-09\n",
      "1.0000000000000005e-09\n",
      "1.0000000000000005e-09\n",
      "1.0000000000000005e-09\n",
      "1.0000000000000005e-09\n",
      "1.0000000000000005e-09\n",
      "1.0000000000000005e-09\n",
      "1.0000000000000005e-09\n",
      "1.0000000000000006e-10\n",
      "1.0000000000000006e-10\n",
      "1.0000000000000006e-10\n",
      "1.0000000000000006e-10\n",
      "1.0000000000000006e-10\n",
      "1.0000000000000006e-10\n",
      "1.0000000000000006e-10\n",
      "1.0000000000000006e-10\n",
      "1.0000000000000006e-10\n",
      "1.0000000000000006e-10\n",
      "1.0000000000000006e-11\n",
      "1.0000000000000006e-11\n",
      "1.0000000000000006e-11\n",
      "1.0000000000000006e-11\n",
      "1.0000000000000006e-11\n",
      "1.0000000000000006e-11\n",
      "1.0000000000000006e-11\n",
      "1.0000000000000006e-11\n",
      "1.0000000000000006e-11\n",
      "1.0000000000000006e-11\n",
      "1.0000000000000006e-12\n",
      "1.0000000000000006e-12\n",
      "1.0000000000000006e-12\n",
      "1.0000000000000006e-12\n",
      "1.0000000000000006e-12\n",
      "1.0000000000000006e-12\n",
      "1.0000000000000006e-12\n",
      "1.0000000000000006e-12\n",
      "1.0000000000000006e-12\n",
      "1.0000000000000006e-12\n",
      "1.0000000000000007e-13\n"
     ]
    }
   ],
   "source": [
    "# Assuming optimizer uses lr = 0.05 for all groups\n",
    "# lr = 0.05     if epoch < 30\n",
    "# lr = 0.005    if 30 <= epoch < 60\n",
    "# lr = 0.0005   if 60 <= epoch < 90\n",
    "# ...\n",
    "from models import ResnetUnet\n",
    "model = ResnetUnet(in_channels=3)\n",
    "params_to_optimize = [{'params': model.parameters()}] \n",
    "optimizer = torch.optim.Adam(params_to_optimize, lr=1e-3, weight_decay=1e-05)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "for epoch in range(100):\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    print(scheduler.get_last_lr()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
