{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6c7cbe6a70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "import torch, gc\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "from models import Unet, get_default_device, to_device, DeviceDataLoader\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L2', 'L1', 'TV', 'L2_L1', 'L2_TV', 'L1_TV', 'L2_L1_TV']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rectype=['L2','L1','TV']\n",
    "rectype_combinations=[] \n",
    "rectype_strings=[]\n",
    "for i in range(len(rectype)):\n",
    "    for p in combinations(rectype, i+1):  # 2 for pairs, 3 for triplets, etc\n",
    "\n",
    "        rectype_combinations.append(p)\n",
    "        rectype_strings.append('_'.join(p))\n",
    "rectype_combinations\n",
    "rectype_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./BIRN_dataset/birn_pngs_20lines_L2/',\n",
       " './BIRN_dataset/birn_pngs_40lines_L2/',\n",
       " './BIRN_dataset/birn_pngs_60lines_L2/',\n",
       " './BIRN_dataset/birn_pngs_80lines_L2/',\n",
       " './BIRN_dataset/birn_pngs_100lines_L2/',\n",
       " './BIRN_dataset/birn_pngs_20lines_L1/',\n",
       " './BIRN_dataset/birn_pngs_40lines_L1/',\n",
       " './BIRN_dataset/birn_pngs_60lines_L1/',\n",
       " './BIRN_dataset/birn_pngs_80lines_L1/',\n",
       " './BIRN_dataset/birn_pngs_100lines_L1/',\n",
       " './BIRN_dataset/birn_pngs_20lines_TV/',\n",
       " './BIRN_dataset/birn_pngs_40lines_TV/',\n",
       " './BIRN_dataset/birn_pngs_60lines_TV/',\n",
       " './BIRN_dataset/birn_pngs_80lines_TV/',\n",
       " './BIRN_dataset/birn_pngs_100lines_TV/']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radial_lines=[20,40,60,80,100]\n",
    "dataset_dir='./BIRN_dataset/'\n",
    "images_dir=(dataset_dir+'birn_png/')\n",
    "rec_dirs=[(f\"{dataset_dir}birn_pngs_{rl}lines_{rt}/\") for rt in rectype for rl in radial_lines]\n",
    "rec_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OriginalReconstructionDataset(Dataset):\n",
    "    def __init__(self, radial_line, rec_type_str, datasets_dir, indexes = None, img_size=(256,256)):\n",
    "        rec_type=rec_type_str.split('_')\n",
    "        self.images_dir=(dataset_dir+'birn_png/')\n",
    "        rec_dirs=[(f\"{dataset_dir}birn_pngs_{rl}lines_{rt}/\") for rt in rectype for rl in radial_lines]\n",
    "        \n",
    "        self.rec_images_dirs=[]\n",
    "        for dir in rec_dirs:\n",
    "            for rt in rec_type:\n",
    "                if rt in dir:\n",
    "                    if str(radial_line) in dir:\n",
    "                        self.rec_images_dirs.append(dir)\n",
    "                        break\n",
    "\n",
    "        self.images = [f for f in os.listdir(self.images_dir) if f.endswith('.png')]\n",
    "        if indexes is not None:\n",
    "            self.images = [self.images[i] for i in indexes] \n",
    "        self.transform = transforms.Compose([\n",
    "                        transforms.Grayscale(num_output_channels=1),         \n",
    "                        transforms.Resize(img_size),\n",
    "                        #transforms.Lambda(lambda x: x/255.0),\n",
    "                        transforms.ToTensor()\n",
    "                        ])\n",
    "        self.rec_types=rec_type\n",
    "        self.radial_line=radial_line\n",
    "        print(self.images_dir)\n",
    "        print(self.rec_images_dirs)\n",
    "        print(self.rec_types)\n",
    "        print(self.radial_line)\n",
    "    def __len__(self):\n",
    "    # return length of image samples    \n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name=self.images[idx]\n",
    "        img = Image.open(self.images_dir+img_name)\n",
    "        img=self.transform(img)\n",
    "        rec_imgs=[]\n",
    "        for rec,dir in zip(self.rec_types,self.rec_images_dirs):\n",
    "            noisy_name=img_name[:-14]+rec+f'_{self.radial_line}lines.png'            \n",
    "            tensor=self.transform(Image.open(dir+noisy_name))\n",
    "            rec_imgs.append(tensor)\n",
    "        noisy=torch.stack(rec_imgs)\n",
    "        noisy=torch.squeeze(noisy, 1)\n",
    "        return (img,noisy)\n",
    "\n",
    "def first_element(test_dataset):\n",
    "    for data in test_dataset:\n",
    "        print(data[0].shape)\n",
    "        print(data[1].shape)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_file='indexes.pkl'\n",
    "if not os.path.exists(idx_file):\n",
    "    np.random.seed(seed=42)\n",
    "    all_indexes=np.random.permutation(len([f for f in os.listdir(images_dir) if f.endswith('.png')]))\n",
    "    m = len(all_indexes)\n",
    "    m_train=int(m*0.8)\n",
    "    m_val = int(m*0.1)\n",
    "    train_indexes=all_indexes[:m_train]\n",
    "    val_indexes=all_indexes[m_train:m_train+m_val]\n",
    "    test_indexes=all_indexes[m_train+m_val:]\n",
    "    \n",
    "    with open(idx_file, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "        pickle.dump([train_indexes, val_indexes, test_indexes], f)\n",
    "else:\n",
    "    with open(idx_file,'rb') as f:  # Python 3: open(..., 'rb')\n",
    "        train_indexes, val_indexes, test_indexes = pickle.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/']\n",
      "['L2']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/']\n",
      "['L2']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/']\n",
      "['L2']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/']\n",
      "['L2']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/']\n",
      "['L2']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L1/']\n",
      "['L1']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L1/']\n",
      "['L1']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L1/']\n",
      "['L1']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L1/']\n",
      "['L1']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L1/']\n",
      "['L1']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/', './BIRN_dataset/birn_pngs_20lines_L1/']\n",
      "['L2', 'L1']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/', './BIRN_dataset/birn_pngs_40lines_L1/']\n",
      "['L2', 'L1']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/', './BIRN_dataset/birn_pngs_60lines_L1/']\n",
      "['L2', 'L1']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/', './BIRN_dataset/birn_pngs_80lines_L1/']\n",
      "['L2', 'L1']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/', './BIRN_dataset/birn_pngs_100lines_L1/']\n",
      "['L2', 'L1']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/', './BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['L2', 'TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/', './BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['L2', 'TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/', './BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['L2', 'TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/', './BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['L2', 'TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/', './BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['L2', 'TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L1/', './BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['L1', 'TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L1/', './BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['L1', 'TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L1/', './BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['L1', 'TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L1/', './BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['L1', 'TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L1/', './BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['L1', 'TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/', './BIRN_dataset/birn_pngs_20lines_L1/', './BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/', './BIRN_dataset/birn_pngs_40lines_L1/', './BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/', './BIRN_dataset/birn_pngs_60lines_L1/', './BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/', './BIRN_dataset/birn_pngs_80lines_L1/', './BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/', './BIRN_dataset/birn_pngs_100lines_L1/', './BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/']\n",
      "['L2']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/']\n",
      "['L2']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/']\n",
      "['L2']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/']\n",
      "['L2']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/']\n",
      "['L2']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L1/']\n",
      "['L1']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L1/']\n",
      "['L1']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L1/']\n",
      "['L1']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L1/']\n",
      "['L1']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L1/']\n",
      "['L1']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/', './BIRN_dataset/birn_pngs_20lines_L1/']\n",
      "['L2', 'L1']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/', './BIRN_dataset/birn_pngs_40lines_L1/']\n",
      "['L2', 'L1']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/', './BIRN_dataset/birn_pngs_60lines_L1/']\n",
      "['L2', 'L1']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/', './BIRN_dataset/birn_pngs_80lines_L1/']\n",
      "['L2', 'L1']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/', './BIRN_dataset/birn_pngs_100lines_L1/']\n",
      "['L2', 'L1']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/', './BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['L2', 'TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/', './BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['L2', 'TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/', './BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['L2', 'TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/', './BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['L2', 'TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/', './BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['L2', 'TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L1/', './BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['L1', 'TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L1/', './BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['L1', 'TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L1/', './BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['L1', 'TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L1/', './BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['L1', 'TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L1/', './BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['L1', 'TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/', './BIRN_dataset/birn_pngs_20lines_L1/', './BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/', './BIRN_dataset/birn_pngs_40lines_L1/', './BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/', './BIRN_dataset/birn_pngs_60lines_L1/', './BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/', './BIRN_dataset/birn_pngs_80lines_L1/', './BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/', './BIRN_dataset/birn_pngs_100lines_L1/', './BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/']\n",
      "['L2']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/']\n",
      "['L2']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/']\n",
      "['L2']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/']\n",
      "['L2']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/']\n",
      "['L2']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L1/']\n",
      "['L1']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L1/']\n",
      "['L1']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L1/']\n",
      "['L1']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L1/']\n",
      "['L1']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L1/']\n",
      "['L1']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/', './BIRN_dataset/birn_pngs_20lines_L1/']\n",
      "['L2', 'L1']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/', './BIRN_dataset/birn_pngs_40lines_L1/']\n",
      "['L2', 'L1']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/', './BIRN_dataset/birn_pngs_60lines_L1/']\n",
      "['L2', 'L1']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/', './BIRN_dataset/birn_pngs_80lines_L1/']\n",
      "['L2', 'L1']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/', './BIRN_dataset/birn_pngs_100lines_L1/']\n",
      "['L2', 'L1']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/', './BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['L2', 'TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/', './BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['L2', 'TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/', './BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['L2', 'TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/', './BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['L2', 'TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/', './BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['L2', 'TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L1/', './BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['L1', 'TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L1/', './BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['L1', 'TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L1/', './BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['L1', 'TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L1/', './BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['L1', 'TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L1/', './BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['L1', 'TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([2, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/', './BIRN_dataset/birn_pngs_20lines_L1/', './BIRN_dataset/birn_pngs_20lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "20\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_40lines_L2/', './BIRN_dataset/birn_pngs_40lines_L1/', './BIRN_dataset/birn_pngs_40lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "40\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_60lines_L2/', './BIRN_dataset/birn_pngs_60lines_L1/', './BIRN_dataset/birn_pngs_60lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "60\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_80lines_L2/', './BIRN_dataset/birn_pngs_80lines_L1/', './BIRN_dataset/birn_pngs_80lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "80\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_100lines_L2/', './BIRN_dataset/birn_pngs_100lines_L1/', './BIRN_dataset/birn_pngs_100lines_TV/']\n",
      "['L2', 'L1', 'TV']\n",
      "100\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "#nao precisa dessa celula\n",
    "batch_size=4\n",
    "device=get_default_device()\n",
    "radial_lines=[20,40,60,80,100]\n",
    "\n",
    "train_dataset={}\n",
    "train_loaders={}\n",
    "for rt in rectype_strings:\n",
    "    for rl in radial_lines:\n",
    "        train_ds=OriginalReconstructionDataset(rl, rt, dataset_dir, train_indexes)\n",
    "        train_dataset[rl,rt]=train_ds\n",
    "        train_loaders[rl,rt]=DeviceDataLoader(torch.utils.data.DataLoader(train_ds, batch_size=batch_size), device)\n",
    "        first_element(train_ds)\n",
    "\n",
    "\n",
    "val_dataset={}\n",
    "val_loaders={}\n",
    "for rt in rectype_strings:\n",
    "    for rl in radial_lines:\n",
    "        val_ds=OriginalReconstructionDataset(rl, rt, dataset_dir, val_indexes)\n",
    "        val_dataset[rl,rt]=val_ds\n",
    "        val_loaders[rl,rt]=DeviceDataLoader(torch.utils.data.DataLoader(val_ds, batch_size=batch_size), device)\n",
    "        first_element(val_ds)\n",
    "\n",
    "test_dataset={}\n",
    "test_loaders={}\n",
    "for rt in rectype_strings:\n",
    "    for rl in radial_lines:\n",
    "        test_ds=OriginalReconstructionDataset(rl, rt, dataset_dir, test_indexes)\n",
    "        test_dataset[rl,rt]=test_ds\n",
    "        test_loaders[rl,rt]=DeviceDataLoader(torch.utils.data.DataLoader(test_ds, batch_size=batch_size,shuffle=True), device)\n",
    "        first_element(test_ds)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Training function\n",
    "def train_epoch_den(model, device, dataloader, loss_fn, optimizer, scheduler):\n",
    "    # Set train mode\n",
    "    model.train()    \n",
    "    train_loss = []\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for image_batch, image_noisy in tqdm(dataloader): # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "        image_noisy.to(device)\n",
    "        result = model(image_noisy)\n",
    "\n",
    "        # Evaluate loss\n",
    "        loss = loss_fn(result, image_batch)\n",
    "        # Backward pass\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "       \n",
    "             \n",
    "        # Print batch loss\n",
    "        #print('\\t partial train loss (single batch): %f' % (loss.data))\n",
    "        train_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "### Testing function\n",
    "def test_epoch_den(model, device, dataloader, loss_fn):\n",
    "    # Set evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # Define the lists to store the outputs for each batch\n",
    "        conc_out = []\n",
    "        conc_label = []\n",
    "        for image_batch, image_noisy in dataloader:\n",
    "            result = model(image_noisy)\n",
    "            # Append the network output and the original image to the lists\n",
    "            conc_out.append(result.cpu())\n",
    "            conc_label.append(image_batch.cpu())\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        conc_out = torch.cat(conc_out)\n",
    "        conc_label = torch.cat(conc_label) \n",
    "        # Evaluate global loss\n",
    "        val_loss = loss_fn(conc_out, conc_label)\n",
    "    return val_loss.data\n",
    "\n",
    "def plot_ae_outputs_den(model,n=10):\n",
    "    plt.figure(figsize=(21,6))\n",
    "    for i in range(n):\n",
    "\n",
    "      ax = plt.subplot(3,n,i+1)\n",
    "      img = test_dataset[4*i][0].unsqueeze(0)\n",
    "      image_noisy = test_dataset[4*i][1].unsqueeze(0)\n",
    "      \n",
    "      model.eval()\n",
    "\n",
    "      with torch.no_grad():\n",
    "         rec_img  = model(image_noisy)\n",
    "\n",
    "      plt.imshow(img.cpu().squeeze().numpy()[0,:,:], cmap='gist_gray')\n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "        ax.set_title('Original images')\n",
    "      ax = plt.subplot(3, n, i + 1 + n)\n",
    "      plt.imshow(image_noisy.cpu().squeeze().numpy()[0,:,:], cmap='gist_gray')\n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "        ax.set_title('Corrupted images')\n",
    "\n",
    "      ax = plt.subplot(3, n, i + 1 + n + n)\n",
    "      plt.imshow(rec_img.cpu().squeeze().numpy()[0,:,:], cmap='gist_gray')  \n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "         ax.set_title('Reconstructed images')\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.7, \n",
    "                    top=0.9, \n",
    "                    wspace=0.3, \n",
    "                    hspace=0.3)   \n",
    "    \n",
    "    plt.savefig('images_256_CS_TV.png')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_optimizer(lr_type, lr, params_to_optimize,max_epochs=500):\n",
    "    if lr_type == 'constant':\n",
    "        optimizer = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
    "        scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer)\n",
    "    if lr_type =='exp':\n",
    "        optimizer  = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.000001**(1/max_epochs), last_epoch=- 1, verbose=False)\n",
    "    elif lr_type == 'plateau':\n",
    "        optimizer = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1**(1/2), patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
    "        scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer)\n",
    "    return (lr_type, optimizer,scheduler)\n",
    "    \n",
    "    \n",
    "def save_model(path, epoch, model, optimizer, scheduler, train_loss, val_loss):\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            }, path)\n",
    "            \n",
    "def optimizer_to(optim, device):\n",
    "    for param in optim.state.values():\n",
    "        # Not sure there are any global tensors in the state dict\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)\n",
    "        elif isinstance(param, dict):\n",
    "            for subparam in param.values():\n",
    "                if isinstance(subparam, torch.Tensor):\n",
    "                    subparam.data = subparam.data.to(device)\n",
    "                    if subparam._grad is not None:\n",
    "                        subparam._grad.data = subparam._grad.data.to(device)\n",
    "\n",
    "def load_model(model_name, rec_type='', lr_type='constant', learning_rate=1e-4, path='', device='cuda:0'):\n",
    "    if model_name=='Unet':\n",
    "        model=Unet(num_inputs=len(rec_type.split('_'))) #1 a 3 canais\n",
    "    elif model_name=='Resnet_Unet':\n",
    "        from models import ResnetUnet\n",
    "        model = ResnetUnet(in_channels=len(rec_type.split('_')))        \n",
    "        #IF TO ADD NEW MODEL IMPLEMENT H#RE\n",
    "    else:\n",
    "        raise Exception('Model not found...')\n",
    "    \n",
    "    params_to_optimize = [{'params': model.parameters()}]        \n",
    "    lr_type, optimizer,scheduler=define_optimizer(lr_type, learning_rate, params_to_optimize)    \n",
    "    epoch = 0\n",
    "    train_loss=float('inf')\n",
    "    val_loss=float('inf')\n",
    "    \n",
    "    if path!='':\n",
    "        checkpoint = torch.load(path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        train_loss = checkpoint['train_loss']\n",
    "        val_loss = checkpoint['val_loss']\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    return (model, optimizer, scheduler, epoch, train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_csv(csv_filename):\n",
    "    if not os.path.exists(csv_filename):\n",
    "        df_history = pd.DataFrame(columns = ['epoch','lr_scheduler','learning_rate','train_loss','val_loss', 'checkpoint', 'status'])\n",
    "        last_epoch=0\n",
    "        last_checkpoint=''\n",
    "        return df_history, last_epoch, last_checkpoint\n",
    "    \n",
    "    folder=csv_filename[:csv_filename.rfind('/')]\n",
    "    #preparing the csv\n",
    "    df_history=pd.read_csv(csv_filename)\n",
    "    df_history = df_history.loc[:, ~df_history.columns.str.contains('^Unnamed')]\n",
    "    df_history=df_history.drop_duplicates(subset=['epoch'], keep='first')\n",
    "    \n",
    "    #find best model (lowest validation loss)\n",
    "    val_sorted_df=df_history.sort_values(by=['val_loss'])\n",
    "    for i, row in val_sorted_df.iterrows():\n",
    "        if os.path.isfile(row['checkpoint']):\n",
    "            best_epoch=row['epoch']\n",
    "            best_checkpoint=row['checkpoint']\n",
    "            best_checkpoint=best_checkpoint.replace('//','/')\n",
    "            break\n",
    "    \n",
    "    #find last saved model\n",
    "    epoch_sorted_df=df_history.sort_values(by=['epoch'], ascending=False)\n",
    "    for i, row in epoch_sorted_df.iterrows():\n",
    "        if os.path.isfile(row['checkpoint']):\n",
    "            last_epoch=row['epoch']\n",
    "            last_checkpoint=row['checkpoint']\n",
    "            last_checkpoint=last_checkpoint.replace('//','/')\n",
    "            break\n",
    "\n",
    "    \n",
    "    #erase rows that do not have a saved model\n",
    "    df_history=df_history[df_history['epoch']<=last_epoch]\n",
    "\n",
    "    files_2_delete=[folder+f for f in os.listdir(folder) if f.endswith('.pth')]\n",
    "    files_2_delete.remove(best_checkpoint)\n",
    "    files_2_delete.remove(last_checkpoint)    \n",
    "\n",
    "    for f in files_2_delete:\n",
    "        #os.remove(f)\n",
    "        pass\n",
    "    \n",
    "    return df_history, last_epoch, last_checkpoint\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def train(rec_type, radial_lines, model_name, loss_fn, lr_type, learning_rate, dl_train, dl_val,device,max_epochs):\n",
    "\n",
    "    \n",
    "    path=f'saved_models/{model_name}_{radial_lines}lines_{rec_type}/'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    csv_filename= f'{path}/loss_history.csv'\n",
    "    df_history, last_epoch, last_checkpoint = check_csv(csv_filename)\n",
    "\n",
    "    (model, optimizer, scheduler, epoch, train_loss, val_loss) = load_model(model_name,  rec_type, lr_type, learning_rate, path=last_checkpoint, device=device)\n",
    "\n",
    "    for epoch in range(epoch,max_epochs):\n",
    "\n",
    "        print('EPOCH %d/%d' % (epoch, max_epochs))        \n",
    "\n",
    "        ### Training (use the training function)\n",
    "        train_loss=train_epoch_den(\n",
    "            model=model, \n",
    "            device=device, \n",
    "            dataloader=dl_train, \n",
    "            loss_fn=loss_fn, \n",
    "            optimizer=optimizer, \n",
    "            scheduler=scheduler)\n",
    "\n",
    "        ### Validation  (use the testing function)\n",
    "        val_loss = test_epoch_den(\n",
    "            model=model, \n",
    "            device=device, \n",
    "            dataloader=dl_val,\n",
    "            loss_fn=loss_fn)\n",
    "        # Print Validationloss\n",
    "\n",
    "\n",
    "        train_loss=train_loss if not torch.is_tensor(train_loss) else train_loss.cpu().detach().numpy()\n",
    "        val_loss=val_loss if not torch.is_tensor(val_loss) else val_loss.cpu().detach().numpy()\n",
    "\n",
    "        filename=f'{path}/{model.name()}_{radial_lines}lines_{rec_type}_epoch{epoch}_lr_{lr_type}_{optimizer.param_groups[0][\"lr\"]}.pth'        \n",
    "        save_model(filename, epoch, model, optimizer, scheduler, train_loss, val_loss)\n",
    "        \n",
    "        if val_loss<=df_history['val_loss'].min():\n",
    "            save_model(filename, epoch, model, optimizer, scheduler, train_loss, val_loss)\n",
    "            status='new best'\n",
    "            df_history.replace(to_replace=\"new best\", value='old best')\n",
    "            best_model=filename\n",
    "        else:\n",
    "            status='never best'    \n",
    "            \n",
    "        df_history = pd.concat([df_history,\n",
    "                                pd.DataFrame({'epoch':[epoch],\n",
    "                                                'lr_scheduler':[lr_type],\n",
    "                                                'learning_rate':[optimizer.param_groups[0][\"lr\"]],\n",
    "                                                'train_loss':[train_loss],\n",
    "                                                'val_loss':[val_loss], \n",
    "                                                'checkpoint':[filename], \n",
    "                                                'status':[status]})])\n",
    "        df_history = df_history.loc[:, ~df_history.columns.str.contains('^Unnamed')]\n",
    "        \n",
    "        df_history.to_csv(csv_filename)       \n",
    "        last_model=filename       \n",
    "\n",
    "\n",
    "    return df_history\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 619 ms, sys: 673 ms, total: 1.29 s\n",
      "Wall time: 500 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import mlflow\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def execute(num_radial_lines, data_train, data_val, data_test, device, exp_params):\n",
    "    experiment_name=f\"MRIREC_{num_radial_lines}\"\n",
    "    run_params = {\"description\":f\"Reconstruction using {num_radial_lines} radial lines\",\n",
    "              \"tags\":{'release.version':'1.0.0'}}\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if not experiment:\n",
    "        experiment_id=mlflow.create_experiment(experiment_name)\n",
    "    experiment = mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    run_params.update({\"experiment_id\": experiment.experiment_id})\n",
    "    \n",
    "    print(\"Experiment_id: {}\".format(experiment.experiment_id))\n",
    "    print(\"Localização dos artefatos: {}\".format(experiment.artifact_location))\n",
    "    print(\"Tags: {}\".format(experiment.tags))\n",
    "    print(\"Lifecycle_stage: {}\".format(experiment.lifecycle_stage))\n",
    "\n",
    "    grid_exp = ParameterGrid(exp_params)\n",
    "\n",
    "    for p_model in grid_exp:\n",
    "        with mlflow.start_run(**run_params) as run:\n",
    "\n",
    "            mlflow.log_params({'model': p_model['model'], 'rectype': p_model['rectype'], 'max_epochs': p_model['max_epochs'], 'learning_rate': p_model['learning_rate'], 'batch_size': p_model['batch_size']})\n",
    "\n",
    "            print(p_model)\n",
    "            model = p_model['model']\n",
    "            rectype = p_model['rectype']\n",
    "            num_channels = len(rectype.split('_'))\n",
    "            epochs = p_model['max_epochs']\n",
    "            lr_type, learning_rate = p_model['learning_rate']\n",
    "            batch_size = p_model['batch_size']\n",
    "\n",
    "            train_ds= OriginalReconstructionDataset(num_radial_lines, rectype, dataset_dir, train_indexes)\n",
    "            train_dl= DeviceDataLoader(torch.utils.data.DataLoader(train_ds, batch_size=batch_size), device)\n",
    "            val_ds=OriginalReconstructionDataset(num_radial_lines, rectype, dataset_dir, test_indexes)\n",
    "            val_dl= DeviceDataLoader(torch.utils.data.DataLoader(train_ds, batch_size=batch_size), device)\n",
    "\n",
    "            #train loop:\n",
    "            history=train(rectype, num_radial_lines, model, torch.nn.MSELoss(), lr_type, learning_rate, train_dl, val_dl, device,epochs)\n",
    "            \n",
    "            #display(history)\n",
    "            #print(history.val_loss.dtype)\n",
    "            #print(type(history.val_loss[0]))\n",
    "            #chosen_idx=history[['val_loss']].idxmin()[0]\n",
    "\n",
    "            #mlflow.log_metric('best_train_loss',history.at[chosen_idx,'train_loss'])\n",
    "            #mlflow.log_metric('best_val_loss',history.at[chosen_idx,'val_loss'])\n",
    "            #mlflow.log_metric('epoch',history.at[chosen_idx,'epoch'])\n",
    "            #mlflow.log_metric('checkpoint',history.at[chosen_idx,'checkpoint'])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment_id: 585268919028592994\n",
      "Localização dos artefatos: file:///home/jonathan/MRI_unet_reconstruction/mlruns/585268919028592994\n",
      "Tags: {}\n",
      "Lifecycle_stage: active\n",
      "{'batch_size': 4, 'learning_rate': ('constant', 0.0001), 'max_epochs': 500, 'model': 'Resnet_Unet', 'rectype': 'L2'}\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/']\n",
      "['L2']\n",
      "20\n",
      "./BIRN_dataset/birn_png/\n",
      "['./BIRN_dataset/birn_pngs_20lines_L2/']\n",
      "['L2']\n",
      "20\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m radial_lines\u001b[39m=\u001b[39m[\u001b[39m20\u001b[39m,\u001b[39m40\u001b[39m,\u001b[39m60\u001b[39m,\u001b[39m80\u001b[39m,\u001b[39m100\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m rl \u001b[39min\u001b[39;00m radial_lines:\n\u001b[0;32m---> 13\u001b[0m     execute(rl, train_loaders, val_loaders, test_loaders,device,exp_params)\n",
      "File \u001b[0;32m<timed exec>:41\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(num_radial_lines, data_train, data_val, data_test, device, exp_params)\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 58\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(rec_type, radial_lines, model_name, loss_fn, lr_type, learning_rate, dl_train, dl_val, device, max_epochs)\u001b[0m\n\u001b[1;32m     55\u001b[0m csv_filename\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m/loss_history.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     56\u001b[0m df_history, last_epoch, last_checkpoint \u001b[39m=\u001b[39m check_csv(csv_filename)\n\u001b[0;32m---> 58\u001b[0m (model, optimizer, scheduler, epoch, train_loss, val_loss) \u001b[39m=\u001b[39m load_model(model_name,  rec_type, lr_type, learning_rate, path\u001b[39m=\u001b[39;49mlast_checkpoint, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch,max_epochs):\n\u001b[1;32m     62\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEPOCH \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (epoch, max_epochs))        \n",
      "Cell \u001b[0;32mIn[9], line 65\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_name, rec_type, lr_type, learning_rate, path, device)\u001b[0m\n\u001b[1;32m     63\u001b[0m     train_loss \u001b[39m=\u001b[39m checkpoint[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     64\u001b[0m     val_loss \u001b[39m=\u001b[39m checkpoint[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 65\u001b[0m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     66\u001b[0m optimizer_to(optimizer,device)\n\u001b[1;32m     67\u001b[0m \u001b[39mreturn\u001b[39;00m (model, optimizer, scheduler, epoch, train_loss, val_loss)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    987\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 989\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 664\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    665\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    666\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    985\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 987\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "exp_params={#\"model\": ['Unet', 'Resnet_Unet'],\n",
    "            \"model\": ['Resnet_Unet'],\n",
    "            \"rectype\": rectype_strings,\n",
    "            \"learning_rate\":[('constant', 1e-4)],#[1e-4],#, 'exp', 'plateau'],\n",
    "            \"max_epochs\":[500],\n",
    "            \"batch_size\": [4],\n",
    "}\n",
    "\n",
    "\n",
    "radial_lines=[20,40,60,80,100]\n",
    "\n",
    "for rl in radial_lines:\n",
    "    execute(rl, train_loaders, val_loaders, test_loaders,device,exp_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "#train_epoch_den\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "dftst=pd.DataFrame({'a':[1,2,-3],'b':[2,3,4]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "chosen_idx=dftst[['a','b']].idxmin()[0]\n",
    "\n",
    "chosen_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
