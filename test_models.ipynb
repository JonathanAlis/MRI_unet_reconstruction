{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Unet, get_default_device, to_device, DeviceDataLoader\n",
    "from torchsummary import summary\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "in_channels=2\n",
    "out_channels=1\n",
    "imsize=256\n",
    "device=get_default_device()\n",
    "input_batch=torch.zeros((batch_size,in_channels,imsize,imsize)).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           1,216\n",
      "       BatchNorm2d-2         [-1, 64, 256, 256]             128\n",
      "         LeakyReLU-3         [-1, 64, 256, 256]               0\n",
      "            Conv2d-4         [-1, 64, 256, 256]          36,928\n",
      "       BatchNorm2d-5         [-1, 64, 256, 256]             128\n",
      "         LeakyReLU-6         [-1, 64, 256, 256]               0\n",
      "            Conv2d-7         [-1, 64, 128, 128]          36,928\n",
      "       BatchNorm2d-8         [-1, 64, 128, 128]             128\n",
      "         LeakyReLU-9         [-1, 64, 128, 128]               0\n",
      "           Conv2d-10        [-1, 128, 128, 128]          73,856\n",
      "      BatchNorm2d-11        [-1, 128, 128, 128]             256\n",
      "        LeakyReLU-12        [-1, 128, 128, 128]               0\n",
      "           Conv2d-13        [-1, 128, 128, 128]         147,584\n",
      "      BatchNorm2d-14        [-1, 128, 128, 128]             256\n",
      "        LeakyReLU-15        [-1, 128, 128, 128]               0\n",
      "           Conv2d-16          [-1, 128, 64, 64]         147,584\n",
      "      BatchNorm2d-17          [-1, 128, 64, 64]             256\n",
      "        LeakyReLU-18          [-1, 128, 64, 64]               0\n",
      "           Conv2d-19          [-1, 256, 64, 64]         295,168\n",
      "      BatchNorm2d-20          [-1, 256, 64, 64]             512\n",
      "        LeakyReLU-21          [-1, 256, 64, 64]               0\n",
      "  ConvTranspose2d-22          [-1, 128, 64, 64]         295,040\n",
      "      BatchNorm2d-23          [-1, 128, 64, 64]             256\n",
      "             ReLU-24          [-1, 128, 64, 64]               0\n",
      "  ConvTranspose2d-25        [-1, 128, 128, 128]         147,584\n",
      "      BatchNorm2d-26        [-1, 128, 128, 128]             256\n",
      "             ReLU-27        [-1, 128, 128, 128]               0\n",
      "  ConvTranspose2d-28        [-1, 128, 128, 128]         295,040\n",
      "      BatchNorm2d-29        [-1, 128, 128, 128]             256\n",
      "             ReLU-30        [-1, 128, 128, 128]               0\n",
      "  ConvTranspose2d-31         [-1, 64, 128, 128]          73,792\n",
      "      BatchNorm2d-32         [-1, 64, 128, 128]             128\n",
      "             ReLU-33         [-1, 64, 128, 128]               0\n",
      "  ConvTranspose2d-34         [-1, 64, 256, 256]          36,928\n",
      "      BatchNorm2d-35         [-1, 64, 256, 256]             128\n",
      "             ReLU-36         [-1, 64, 256, 256]               0\n",
      "  ConvTranspose2d-37         [-1, 64, 256, 256]          73,792\n",
      "      BatchNorm2d-38         [-1, 64, 256, 256]             128\n",
      "             ReLU-39         [-1, 64, 256, 256]               0\n",
      "  ConvTranspose2d-40         [-1, 64, 256, 256]          36,928\n",
      "      BatchNorm2d-41         [-1, 64, 256, 256]             128\n",
      "             ReLU-42         [-1, 64, 256, 256]               0\n",
      "  ConvTranspose2d-43          [-1, 1, 256, 256]             577\n",
      "      BatchNorm2d-44          [-1, 1, 256, 256]               2\n",
      "             ReLU-45          [-1, 1, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 1,701,891\n",
      "Trainable params: 1,701,891\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.50\n",
      "Forward/backward pass size (MB): 769.50\n",
      "Params size (MB): 6.49\n",
      "Estimated Total Size (MB): 776.49\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "unet = to_device(Unet(num_inputs=in_channels),device)\n",
    "s=summary(unet,input_size=(in_channels,256,256), device = device.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "out_batch=unet(input_batch)\n",
    "print(input_batch.shape)\n",
    "print(out_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 256, 256])\n",
      "torch.Size([2, 64, 256, 256])\n",
      "torch.Size([2, 64, 256, 256])\n",
      "----------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           1,216\n",
      "              ReLU-2         [-1, 64, 256, 256]               0\n",
      "            Conv2d-3         [-1, 64, 256, 256]          36,928\n",
      "              ReLU-4         [-1, 64, 256, 256]               0\n",
      "            Conv2d-5         [-1, 64, 128, 128]           6,272\n",
      "            Conv2d-6         [-1, 64, 128, 128]           6,272\n",
      "       BatchNorm2d-7         [-1, 64, 128, 128]             128\n",
      "       BatchNorm2d-8         [-1, 64, 128, 128]             128\n",
      "              ReLU-9         [-1, 64, 128, 128]               0\n",
      "             ReLU-10         [-1, 64, 128, 128]               0\n",
      "        MaxPool2d-11           [-1, 64, 64, 64]               0\n",
      "        MaxPool2d-12           [-1, 64, 64, 64]               0\n",
      "           Conv2d-13           [-1, 64, 64, 64]          36,864\n",
      "           Conv2d-14           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 64, 64]             128\n",
      "      BatchNorm2d-16           [-1, 64, 64, 64]             128\n",
      "             ReLU-17           [-1, 64, 64, 64]               0\n",
      "             ReLU-18           [-1, 64, 64, 64]               0\n",
      "           Conv2d-19           [-1, 64, 64, 64]          36,864\n",
      "           Conv2d-20           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 64, 64]             128\n",
      "      BatchNorm2d-22           [-1, 64, 64, 64]             128\n",
      "             ReLU-23           [-1, 64, 64, 64]               0\n",
      "             ReLU-24           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-25           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-26           [-1, 64, 64, 64]               0\n",
      "           Conv2d-27           [-1, 64, 64, 64]          36,864\n",
      "           Conv2d-28           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-29           [-1, 64, 64, 64]             128\n",
      "      BatchNorm2d-30           [-1, 64, 64, 64]             128\n",
      "             ReLU-31           [-1, 64, 64, 64]               0\n",
      "             ReLU-32           [-1, 64, 64, 64]               0\n",
      "           Conv2d-33           [-1, 64, 64, 64]          36,864\n",
      "           Conv2d-34           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-35           [-1, 64, 64, 64]             128\n",
      "      BatchNorm2d-36           [-1, 64, 64, 64]             128\n",
      "             ReLU-37           [-1, 64, 64, 64]               0\n",
      "             ReLU-38           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-39           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-40           [-1, 64, 64, 64]               0\n",
      "           Conv2d-41          [-1, 128, 32, 32]          73,728\n",
      "           Conv2d-42          [-1, 128, 32, 32]          73,728\n",
      "      BatchNorm2d-43          [-1, 128, 32, 32]             256\n",
      "      BatchNorm2d-44          [-1, 128, 32, 32]             256\n",
      "             ReLU-45          [-1, 128, 32, 32]               0\n",
      "             ReLU-46          [-1, 128, 32, 32]               0\n",
      "           Conv2d-47          [-1, 128, 32, 32]         147,456\n",
      "           Conv2d-48          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-49          [-1, 128, 32, 32]             256\n",
      "      BatchNorm2d-50          [-1, 128, 32, 32]             256\n",
      "           Conv2d-51          [-1, 128, 32, 32]           8,192\n",
      "           Conv2d-52          [-1, 128, 32, 32]           8,192\n",
      "      BatchNorm2d-53          [-1, 128, 32, 32]             256\n",
      "      BatchNorm2d-54          [-1, 128, 32, 32]             256\n",
      "             ReLU-55          [-1, 128, 32, 32]               0\n",
      "             ReLU-56          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-57          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-58          [-1, 128, 32, 32]               0\n",
      "           Conv2d-59          [-1, 128, 32, 32]         147,456\n",
      "           Conv2d-60          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-61          [-1, 128, 32, 32]             256\n",
      "      BatchNorm2d-62          [-1, 128, 32, 32]             256\n",
      "             ReLU-63          [-1, 128, 32, 32]               0\n",
      "             ReLU-64          [-1, 128, 32, 32]               0\n",
      "           Conv2d-65          [-1, 128, 32, 32]         147,456\n",
      "           Conv2d-66          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-67          [-1, 128, 32, 32]             256\n",
      "      BatchNorm2d-68          [-1, 128, 32, 32]             256\n",
      "             ReLU-69          [-1, 128, 32, 32]               0\n",
      "             ReLU-70          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-71          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-72          [-1, 128, 32, 32]               0\n",
      "           Conv2d-73          [-1, 256, 16, 16]         294,912\n",
      "           Conv2d-74          [-1, 256, 16, 16]         294,912\n",
      "      BatchNorm2d-75          [-1, 256, 16, 16]             512\n",
      "      BatchNorm2d-76          [-1, 256, 16, 16]             512\n",
      "             ReLU-77          [-1, 256, 16, 16]               0\n",
      "             ReLU-78          [-1, 256, 16, 16]               0\n",
      "           Conv2d-79          [-1, 256, 16, 16]         589,824\n",
      "           Conv2d-80          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-81          [-1, 256, 16, 16]             512\n",
      "      BatchNorm2d-82          [-1, 256, 16, 16]             512\n",
      "           Conv2d-83          [-1, 256, 16, 16]          32,768\n",
      "           Conv2d-84          [-1, 256, 16, 16]          32,768\n",
      "      BatchNorm2d-85          [-1, 256, 16, 16]             512\n",
      "      BatchNorm2d-86          [-1, 256, 16, 16]             512\n",
      "             ReLU-87          [-1, 256, 16, 16]               0\n",
      "             ReLU-88          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-89          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-90          [-1, 256, 16, 16]               0\n",
      "           Conv2d-91          [-1, 256, 16, 16]         589,824\n",
      "           Conv2d-92          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-93          [-1, 256, 16, 16]             512\n",
      "      BatchNorm2d-94          [-1, 256, 16, 16]             512\n",
      "             ReLU-95          [-1, 256, 16, 16]               0\n",
      "             ReLU-96          [-1, 256, 16, 16]               0\n",
      "           Conv2d-97          [-1, 256, 16, 16]         589,824\n",
      "           Conv2d-98          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-99          [-1, 256, 16, 16]             512\n",
      "     BatchNorm2d-100          [-1, 256, 16, 16]             512\n",
      "            ReLU-101          [-1, 256, 16, 16]               0\n",
      "            ReLU-102          [-1, 256, 16, 16]               0\n",
      "      BasicBlock-103          [-1, 256, 16, 16]               0\n",
      "      BasicBlock-104          [-1, 256, 16, 16]               0\n",
      "          Conv2d-105            [-1, 512, 8, 8]       1,179,648\n",
      "          Conv2d-106            [-1, 512, 8, 8]       1,179,648\n",
      "     BatchNorm2d-107            [-1, 512, 8, 8]           1,024\n",
      "     BatchNorm2d-108            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-109            [-1, 512, 8, 8]               0\n",
      "            ReLU-110            [-1, 512, 8, 8]               0\n",
      "          Conv2d-111            [-1, 512, 8, 8]       2,359,296\n",
      "          Conv2d-112            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-113            [-1, 512, 8, 8]           1,024\n",
      "     BatchNorm2d-114            [-1, 512, 8, 8]           1,024\n",
      "          Conv2d-115            [-1, 512, 8, 8]         131,072\n",
      "          Conv2d-116            [-1, 512, 8, 8]         131,072\n",
      "     BatchNorm2d-117            [-1, 512, 8, 8]           1,024\n",
      "     BatchNorm2d-118            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-119            [-1, 512, 8, 8]               0\n",
      "            ReLU-120            [-1, 512, 8, 8]               0\n",
      "      BasicBlock-121            [-1, 512, 8, 8]               0\n",
      "      BasicBlock-122            [-1, 512, 8, 8]               0\n",
      "          Conv2d-123            [-1, 512, 8, 8]       2,359,296\n",
      "          Conv2d-124            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-125            [-1, 512, 8, 8]           1,024\n",
      "     BatchNorm2d-126            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-127            [-1, 512, 8, 8]               0\n",
      "            ReLU-128            [-1, 512, 8, 8]               0\n",
      "          Conv2d-129            [-1, 512, 8, 8]       2,359,296\n",
      "          Conv2d-130            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-131            [-1, 512, 8, 8]           1,024\n",
      "     BatchNorm2d-132            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-133            [-1, 512, 8, 8]               0\n",
      "            ReLU-134            [-1, 512, 8, 8]               0\n",
      "      BasicBlock-135            [-1, 512, 8, 8]               0\n",
      "      BasicBlock-136            [-1, 512, 8, 8]               0\n",
      "          Conv2d-137            [-1, 512, 8, 8]         262,656\n",
      "            ReLU-138            [-1, 512, 8, 8]               0\n",
      "        Upsample-139          [-1, 512, 16, 16]               0\n",
      "          Conv2d-140          [-1, 256, 16, 16]          65,792\n",
      "            ReLU-141          [-1, 256, 16, 16]               0\n",
      "          Conv2d-142          [-1, 512, 16, 16]       3,539,456\n",
      "            ReLU-143          [-1, 512, 16, 16]               0\n",
      "        Upsample-144          [-1, 512, 32, 32]               0\n",
      "          Conv2d-145          [-1, 128, 32, 32]          16,512\n",
      "            ReLU-146          [-1, 128, 32, 32]               0\n",
      "          Conv2d-147          [-1, 256, 32, 32]       1,474,816\n",
      "            ReLU-148          [-1, 256, 32, 32]               0\n",
      "        Upsample-149          [-1, 256, 64, 64]               0\n",
      "          Conv2d-150           [-1, 64, 64, 64]           4,160\n",
      "            ReLU-151           [-1, 64, 64, 64]               0\n",
      "          Conv2d-152          [-1, 256, 64, 64]         737,536\n",
      "            ReLU-153          [-1, 256, 64, 64]               0\n",
      "        Upsample-154        [-1, 256, 128, 128]               0\n",
      "          Conv2d-155         [-1, 64, 128, 128]           4,160\n",
      "            ReLU-156         [-1, 64, 128, 128]               0\n",
      "          Conv2d-157        [-1, 128, 128, 128]         368,768\n",
      "            ReLU-158        [-1, 128, 128, 128]               0\n",
      "        Upsample-159        [-1, 128, 256, 256]               0\n",
      "          Conv2d-160         [-1, 64, 256, 256]         110,656\n",
      "            ReLU-161         [-1, 64, 256, 256]               0\n",
      "          Conv2d-162          [-1, 1, 256, 256]              65\n",
      "================================================================\n",
      "Total params: 28,969,473\n",
      "Trainable params: 28,969,473\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.50\n",
      "Forward/backward pass size (MB): 543.00\n",
      "Params size (MB): 110.51\n",
      "Estimated Total Size (MB): 654.01\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from models import ResnetUnet\n",
    "unet_resnet = to_device(ResnetUnet(in_channels=in_channels, out_channels=out_channels),device)\n",
    "s=summary(unet_resnet,input_size=(in_channels,imsize, imsize), device= device.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 256, 256])\n",
      "torch.Size([4, 64, 256, 256])\n",
      "torch.Size([4, 64, 256, 256])\n",
      "----------\n",
      "torch.Size([4, 2, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "out_batch=unet_resnet(input_batch)\n",
    "print(input_batch.shape)\n",
    "print(out_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNBlock(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "    (1): Permute()\n",
      "    (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (3): Linear(in_features=192, out_features=768, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Linear(in_features=768, out_features=192, bias=True)\n",
      "    (6): Permute()\n",
      "  )\n",
      "  (stochastic_depth): StochasticDepth(p=0.005714285714285714, mode=row)\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [192, 3, 4, 4], expected input[4, 2, 256, 256] to have 3 channels, but got 2 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m ConvNextUnet, create_convnext_unet\n\u001b[1;32m      2\u001b[0m convnextunet\u001b[39m=\u001b[39mto_device(create_convnext_unet(), device)\n\u001b[0;32m----> 3\u001b[0m out_batch\u001b[39m=\u001b[39mconvnextunet(input_batch)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(input_batch\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(out_batch\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/MRI_unet_reconstruction/models/convnext_unet.py:753\u001b[0m, in \u001b[0;36mConvNextUnet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    752\u001b[0m     input_ \u001b[39m=\u001b[39m x\n\u001b[0;32m--> 753\u001b[0m     x, blocks \u001b[39m=\u001b[39m get_blocks_to_be_concat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder, x)\n\u001b[1;32m    755\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_conv1(x)\n\u001b[1;32m    756\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x, blocks[\u001b[39m\"\u001b[39m\u001b[39mfeat3\u001b[39m\u001b[39m\"\u001b[39m]], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/MRI_unet_reconstruction/models/convnext_unet.py:703\u001b[0m, in \u001b[0;36mget_blocks_to_be_concat\u001b[0;34m(model, x)\u001b[0m\n\u001b[1;32m    699\u001b[0m model[\u001b[39m5\u001b[39m][\u001b[39m8\u001b[39m]\u001b[39m.\u001b[39mregister_forward_hook(get_features(\u001b[39m'\u001b[39m\u001b[39mfeat3\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m    702\u001b[0m \u001b[39m# make a forward pass to trigger the hooks\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m x \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m    704\u001b[0m \u001b[39m# print (features[\"feat1\"].size())\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[39m# print (features[\"feat2\"].size())\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[39m# print (features[\"feat3\"].size())\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[39mreturn\u001b[39;00m x, features\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [192, 3, 4, 4], expected input[4, 2, 256, 256] to have 3 channels, but got 2 channels instead"
     ]
    }
   ],
   "source": [
    "from models import ConvNextUnet, create_convnext_unet\n",
    "convnextunet=to_device(create_convnext_unet(), device)\n",
    "out_batch=convnextunet(input_batch)\n",
    "print(input_batch.shape)\n",
    "print(out_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
